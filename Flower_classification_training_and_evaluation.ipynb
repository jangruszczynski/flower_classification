{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493339e8",
      "metadata": {
        "id": "493339e8"
      },
      "outputs": [],
      "source": [
        "### Flower classification, Jan Gruszczynski, 11.05.2022\n",
        "### CODE FOR DATASET WHICH IS ALREADY USING THE FOLDER STRUCTURE AS BELOW\n",
        "\n",
        "#- prepared_dataset\n",
        "#-- training\n",
        "#--- flower_1\n",
        "#--- flower_2 ...\n",
        "\n",
        "# Download libraries\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0CYpl-1TGF_7"
      },
      "id": "0CYpl-1TGF_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/drive/MyDrive/Colab Notebooks/prepared_dataset_1.zip (Unzipped Files)/prepared_dataset_1\")"
      ],
      "metadata": {
        "id": "5D1QzhbjGR9x"
      },
      "id": "5D1QzhbjGR9x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e967571d",
      "metadata": {
        "id": "e967571d"
      },
      "outputs": [],
      "source": [
        "# Script to download dataset\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/17/17flowers.tgz\"\n",
        "DATA_PATH = os.path.join(\"datasets\", \"flowers\")\n",
        "IMAGES_PATH = os.path.join(\"datasets\", \"flowers\", \"jpg\")\n",
        "def fetch_housing_data(data_url=DOWNLOAD_ROOT, data_path=DATA_PATH):\n",
        "    if not os.path.isdir(data_path):\n",
        "        os.makedirs(data_path)\n",
        "    tgz_path = os.path.join(data_path, \"17flowers.tgz\")\n",
        "    urllib.request.urlretrieve(data_url, tgz_path)\n",
        "    data_tgz = tarfile.open(tgz_path)\n",
        "    data_tgz.extractall(path=data_path)\n",
        "    data_tgz.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c0925f",
      "metadata": {
        "id": "74c0925f"
      },
      "outputs": [],
      "source": [
        "### PREPARING TRAINING AND TEST DATA\n",
        "\n",
        "# assign labels to subsequent classes\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Colab Notebooks/prepared_dataset_1.zip (Unzipped Files)/prepared_dataset_1\"\n",
        "\n",
        "classes = os.listdir(DATASET_PATH)\n",
        "#print(classes)\n",
        "\n",
        "# prepare training, validation, test data PATHS\n",
        " \n",
        "LIST_FILE_PATH = os.path.join(DATASET_PATH, \"files.txt\")\n",
        "#DATASET_PATH = os.path.join(DATA_PATH, \"prepared_dataset_1\")\n",
        "\n",
        "\n",
        "TRAINING_PATH = os.path.join(DATASET_PATH, \"training\")\n",
        "VALIDATION_PATH = os.path.join(DATASET_PATH, \"validation\")\n",
        "TEST_PATH = os.path.join(DATASET_PATH, \"test\")\n",
        "\n",
        "\n",
        "#os.mkdir(TRAINING_PATH)\n",
        "#os.mkdir(VALIDATION_PATH)\n",
        "#os.mkdir(TEST_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73be0038",
      "metadata": {
        "id": "73be0038"
      },
      "outputs": [],
      "source": [
        "# reading list of images\n",
        "\n",
        "with open(LIST_FILE_PATH) as file:\n",
        "    all_files_list = file.readlines()\n",
        "    all_files_list = [line.rstrip() for line in all_files_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3675ff4",
      "metadata": {
        "id": "f3675ff4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# create list of files\n",
        "\n",
        "with open(LIST_FILE_PATH, \"r\") as file:\n",
        "    file_list = file.readlines()\n",
        "    file_list = [line.rstrip() for line in file_list]\n",
        "    \n",
        "# list of divisible by lenght of class\n",
        "\n",
        "list_classes = [i for i in range(1, len(file_kist)) if i % 80 == 0]\n",
        "list_classes\n",
        "\n",
        "# create datasets\n",
        "\n",
        "#- prepared_dataset\n",
        "#-- training\n",
        "#--- flower_1\n",
        "#--- flower_2 ...\n",
        "\n",
        "# declare location of the resized images\n",
        "\n",
        "IMAGES_PATH = os.path.join(\"datasets\", \"flowers\", \"jpg1\")\n",
        "\n",
        "\n",
        "for i in range(1,18):\n",
        "    \n",
        "    TRAIN_DIR = os.path.join(TRAINING_PATH, \"flower_{}\".format(i))\n",
        "    os.mkdir(TRAIN_DIR)\n",
        "    VAL_DIR = os.path.join(VALIDATION_PATH, \"flower_{}\".format(i))\n",
        "    os.mkdir(VAL_DIR)\n",
        "    TEST_DIR = os.path.join(TEST_PATH, \"flower_{}\".format(i))\n",
        "    os.mkdir(TEST_DIR)\n",
        "    #print(TRAIN_DIR, VAL_DIR, TEST_DIR)\n",
        "    \n",
        "    a = 80*i\n",
        "    print(a)\n",
        "    \n",
        "   \n",
        "    # list of all images in one class\n",
        "    list_total = [j for j in file_list if a-80 < int(re.search(r'\\d+', j).group()) <= a]\n",
        "    random.shuffle(list_total)\n",
        "\n",
        "    training_dataset, test_dataset = sklearn.model_selection.train_test_split(list_total, test_size=20)\n",
        "    test_dataset, valid_dataset = sklearn.model_selection.train_test_split(test_dataset, test_size=10)\n",
        "    #print(\"TRAINING\", len(training_dataset), \"VALID\", len(valid_dataset), \"TEST\", len(test_dataset))\n",
        "    #print(\"TRAINING\", training_dataset, \"VALID\", valid_dataset, \"TEST\", test_dataset)\n",
        "    \n",
        "    #copying files into subsequent datasets\n",
        "    \n",
        "    # CAN BE REPLACED WITH FUNCTION\n",
        "       \n",
        "    #training dataset\n",
        "    src_files = os.listdir(IMAGES_PATH)\n",
        "    for file_name in training_dataset:\n",
        "        full_file_name = os.path.join(IMAGES_PATH, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, TRAIN_DIR)\n",
        "    \n",
        "    #valid dataset\n",
        "    for file_name in valid_dataset:\n",
        "        full_file_name = os.path.join(IMAGES_PATH, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, VAL_DIR)\n",
        "    \n",
        "    #test dataset\n",
        "    for file_name in test_dataset:\n",
        "        full_file_name = os.path.join(IMAGES_PATH, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, TEST_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b700280",
      "metadata": {
        "id": "4b700280"
      },
      "outputs": [],
      "source": [
        "### Exploring Dataset\n",
        "\n",
        "classes = os.listdir(TRAINING_PATH)\n",
        "print(\"Total Classes: \",len(classes))\n",
        "\n",
        "#Counting total train, valid & test images\n",
        "\n",
        "train_count = 0\n",
        "valid_count = 0\n",
        "test_count = 0\n",
        "for _class in classes:\n",
        "    train_count += len(os.listdir(TRAINING_PATH + \"/\" + _class))\n",
        "    valid_count += len(os.listdir(VALIDATION_PATH + \"/\" +_class))\n",
        "    test_count += len(os.listdir(TEST_PATH + \"/\" +_class))\n",
        "\n",
        "print(\"Total train images: \",train_count)\n",
        "print(\"Total valid images: \",valid_count)\n",
        "print(\"Total test images: \",test_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f3a374",
      "metadata": {
        "id": "76f3a374"
      },
      "outputs": [],
      "source": [
        "train_imgs = []\n",
        "valid_imgs = []\n",
        "test_imgs = []\n",
        "\n",
        "for _class in classes:\n",
        "    \n",
        "    for img in os.listdir(TRAINING_PATH + \"/\" +_class):\n",
        "        train_imgs.append(TRAINING_PATH + \"/\" + _class + \"/\" + img)\n",
        "    \n",
        "    for img in os.listdir(VALIDATION_PATH +\"/\" + _class):\n",
        "        valid_imgs.append(VALIDATION_PATH +\"/\" + _class + \"/\" + img)\n",
        "        \n",
        "    for img in os.listdir(TEST_PATH + \"/\" + _class):\n",
        "        test_imgs.append(TEST_PATH + \"/\" +_class + \"/\" + img)\n",
        "\n",
        "class_to_int = {classes[i] : i for i in range(len(classes))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e42ffdc",
      "metadata": {
        "id": "6e42ffdc"
      },
      "outputs": [],
      "source": [
        "### Loading Classification Dataset - FOR METHOD 2: For multi-class data, by inheriting Dataset class\n",
        "\n",
        "def get_transform():\n",
        "    return T.Compose([T.ToTensor()])\n",
        "\n",
        "class FlowerDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, imgs_list, class_to_int, transforms = None):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.imgs_list = imgs_list\n",
        "        self.class_to_int = class_to_int\n",
        "        self.transforms = transforms\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "    \n",
        "        image_path = self.imgs_list[index]\n",
        "        \n",
        "        #Reading image\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        #Retrieving class label\n",
        "        label = image_path.split()[-2]\n",
        "        label = self.class_to_int[label]\n",
        "        \n",
        "        #Applying transforms on image\n",
        "        if self.transforms:\n",
        "            \n",
        "            image = self.transforms(image)\n",
        "        \n",
        "        return image, label\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.imgs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef5c4c9",
      "metadata": {
        "id": "0ef5c4c9"
      },
      "outputs": [],
      "source": [
        "### Loading Classification Dataset\n",
        "\n",
        "\n",
        "# Method 1: For multi-class data directly from folders using ImageFolder\n",
        "train_dataset = ImageFolder(root = TRAINING_PATH, transform = T.ToTensor())\n",
        "valid_dataset = ImageFolder(root = VALIDATION_PATH, transform = T.ToTensor())\n",
        "test_dataset = ImageFolder(root = TEST_PATH, transform = T.ToTensor())\n",
        "\n",
        "\"\"\"\n",
        "# Method 2: Using Dataset Class\n",
        "train_dataset = FlowerDataset(train_imgs, class_to_int, get_transform())\n",
        "valid_dataset = FlowerDataset(valid_imgs, class_to_int, get_transform())\n",
        "test_dataset = FlowerDataset(test_imgs, class_to_int, get_transform())\n",
        "\"\"\"\n",
        "#Data Loader  -  using Sampler (YT Video)\n",
        "train_random_sampler = RandomSampler(train_dataset)\n",
        "valid_random_sampler = RandomSampler(valid_dataset)\n",
        "test_random_sampler = RandomSampler(test_dataset)\n",
        "\n",
        "#Shuffle Argument is mutually exclusive with Sampler!\n",
        "train_data_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = 16,\n",
        "    sampler = train_random_sampler,\n",
        "    num_workers = 0,\n",
        ")\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    dataset = valid_dataset,\n",
        "    batch_size = 16,\n",
        "    sampler = valid_random_sampler,\n",
        "    num_workers = 0\n",
        ")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = 16,\n",
        "    sampler = test_random_sampler,\n",
        "    num_workers = 0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3097933",
      "metadata": {
        "id": "b3097933"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ed3bd0",
      "metadata": {
        "id": "35ed3bd0"
      },
      "outputs": [],
      "source": [
        "# Visualize one training batch\n",
        "for images, labels in train_data_loader:\n",
        "    fig, ax = plt.subplots(figsize = (10, 10))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(make_grid(images, 4).permute(1,2,0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b20592f5",
      "metadata": {
        "id": "b20592f5"
      },
      "outputs": [],
      "source": [
        "### Define model\n",
        "model = models.vgg16(pretrained = True)\n",
        "\n",
        "### Modifying last few layers and no of classes\n",
        "# NOTE: cross_entropy loss takes unnormalized op (logits), then function itself applies softmax and calculates loss, so no need to include softmax here\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096, bias = True),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(4096, 2048, bias = True),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(2048, 200)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c136269b",
      "metadata": {
        "id": "c136269b"
      },
      "outputs": [],
      "source": [
        "### Get device\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "### Training Details\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.75)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loss = []\n",
        "train_accuracy = []\n",
        "\n",
        "val_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf41347",
      "metadata": {
        "id": "5cf41347"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(true,pred):\n",
        "    pred = F.softmax(pred, dim = 1)\n",
        "    true = torch.zeros(pred.shape[0], pred.shape[1]).scatter_(1, true.unsqueeze(1), 1.)\n",
        "    acc = (true.argmax(-1) == pred.argmax(-1)).float().detach().numpy()\n",
        "    acc = float((100 * acc.sum()) / len(acc))\n",
        "    return round(acc, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ce9764",
      "metadata": {
        "id": "e4ce9764"
      },
      "outputs": [],
      "source": [
        "### Training Code\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    #Epoch Loss & Accuracy\n",
        "    train_epoch_loss = []\n",
        "    train_epoch_accuracy = []\n",
        "    _iter = 1\n",
        "    \n",
        "    #Val Loss & Accuracy\n",
        "    val_epoch_loss = []\n",
        "    val_epoch_accuracy = []\n",
        "    \n",
        "    # Training\n",
        "    for images, labels in train_data_loader:\n",
        "        \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        #Reset Grads\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #Forward ->\n",
        "        preds = model(images)\n",
        "        \n",
        "        #Calculate Accuracy\n",
        "        acc = calc_accuracy(labels.cpu(), preds.cpu())\n",
        "        \n",
        "        #Calculate Loss & Backward, Update Weights (Step)\n",
        "        loss = criterion(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #Append loss & acc\n",
        "        loss_value = loss.item()\n",
        "        train_epoch_loss.append(loss_value)\n",
        "        train_epoch_accuracy.append(acc)\n",
        "        \n",
        "        if _iter % 500 == 0:\n",
        "            print(\"> Iteration {} < \".format(_iter))\n",
        "            print(\"Iter Loss = {}\".format(round(loss_value, 4)))\n",
        "            print(\"Iter Accuracy = {} % \\n\".format(acc))\n",
        "        \n",
        "        _iter += 1\n",
        "    \n",
        "    #Validation\n",
        "    for images, labels in valid_data_loader:\n",
        "        \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        #Forward ->\n",
        "        preds = model(images)\n",
        "        \n",
        "        #Calculate Accuracy\n",
        "        acc = calc_accuracy(labels.cpu(), preds.cpu())\n",
        "        \n",
        "        #Calculate Loss\n",
        "        loss = criterion(preds, labels)\n",
        "        \n",
        "        #Append loss & acc\n",
        "        loss_value = loss.item()\n",
        "        val_epoch_loss.append(loss_value)\n",
        "        val_epoch_accuracy.append(acc)\n",
        "    \n",
        "    \n",
        "    train_epoch_loss = np.mean(train_epoch_loss)\n",
        "    train_epoch_accuracy = np.mean(train_epoch_accuracy)\n",
        "    \n",
        "    val_epoch_loss = np.mean(val_epoch_loss)\n",
        "    val_epoch_accuracy = np.mean(val_epoch_accuracy)\n",
        "    \n",
        "    end = time.time()\n",
        "    \n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracy.append(train_epoch_accuracy)\n",
        "    \n",
        "    val_loss.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "    \n",
        "    #Print Epoch Statistics\n",
        "    print(\"** Epoch {} ** - Epoch Time {}\".format(epoch, int(end-start)))\n",
        "    print(\"Train Loss = {}\".format(round(train_epoch_loss, 4)))\n",
        "    print(\"Train Accuracy = {} % \\n\".format(train_epoch_accuracy))\n",
        "    print(\"Val Loss = {}\".format(round(val_epoch_loss, 4)))\n",
        "    print(\"Val Accuracy = {} % \\n\".format(val_epoch_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the model (not working on my drive)\n",
        "def saveModel(): \n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks//prepared_dataset_1.zip (Unzipped Files)/prepared_dataset_1/NetModel_appsilon_flowers_vgg16.pth\"\n",
        "    torch.save(model.state_dict(), path) \n",
        "\n",
        "saveModel()"
      ],
      "metadata": {
        "id": "5Ksx9vjNKjnS"
      },
      "id": "5Ksx9vjNKjnS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define test function \n",
        "\n",
        "def test_funct(test_data_loader):\n",
        "    #Test\n",
        "    for images, labels in test_data_loader:\n",
        "        \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        val_epoch_loss = []\n",
        "        val_epoch_accuracy = []\n",
        "\n",
        "        #Forward ->\n",
        "        preds = model(images)\n",
        "        \n",
        "        #Calculate Accuracy\n",
        "        acc = calc_accuracy(labels.cpu(), preds.cpu())\n",
        "        \n",
        "        #Calculate Loss\n",
        "        loss = criterion(preds, labels)\n",
        "        \n",
        "        #Append loss & acc\n",
        "        loss_value = loss.item()\n",
        "        val_epoch_loss.append(loss_value)\n",
        "        val_epoch_accuracy.append(acc)\n",
        "\n",
        "    val_epoch_loss = np.mean(val_epoch_loss)\n",
        "    val_epoch_accuracy = np.mean(val_epoch_accuracy)\n",
        "\n",
        "    return(val_epoch_loss, val_epoch_accuracy)\n",
        "\n",
        "\n",
        "test_funct(test_data_loader)\n",
        "print(\"avg_loss\", val_epoch_loss, \"avg_accuracy\", val_epoch_accuracy)"
      ],
      "metadata": {
        "id": "Rfqa1nbQbjfy"
      },
      "id": "Rfqa1nbQbjfy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one way to generate predictions list\n",
        "\n",
        "y_pred_list = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for X_batch, _ in test_data_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_test_pred = model(X_batch)\n",
        "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
        "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "metadata": {
        "id": "aTh583KGW1fn"
      },
      "id": "aTh583KGW1fn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label\n",
        "test_features, test_labels = next(iter(test_data_loader))\n",
        "print(f\"Feature batch shape: {test_features.size()}\")\n",
        "print(f\"Labels batch shape: {test_labels.size()}\")\n",
        "img = test_features[0].squeeze()\n",
        "label = test_labels[0]\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "jW18rRKmY_NY"
      },
      "id": "jW18rRKmY_NY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing original labels\n",
        "\n",
        "original_test = []\n",
        "for images, labels in test_data_loader:\n",
        "    fig, ax = plt.subplots(figsize = (10, 10))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(make_grid(images, 4).permute(1,2,0))\n",
        "    #print(labels)\n",
        "    list_temp = labels.tolist()\n",
        "    original_test.append(list_temp)\n",
        "print(original_test)"
      ],
      "metadata": {
        "id": "3l8HRJjzZjs9"
      },
      "id": "3l8HRJjzZjs9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing test labels\n",
        "\n",
        "for batch in test_data_loader:\n",
        "    inputs, targets = batch\n",
        "    for img in inputs:\n",
        "        image  = img.cpu().numpy()\n",
        "        # transpose image to fit plt input\n",
        "        image = image.T\n",
        "        # normalise image\n",
        "        data_min = np.min(image, axis=(1,2), keepdims=True)\n",
        "        data_max = np.max(image, axis=(1,2), keepdims=True)\n",
        "        scaled_data = (image - data_min) / (data_max - data_min)\n",
        "        # show image\n",
        "        plt.imshow(scaled_data)\n",
        "        plt.show()\n",
        "    break"
      ],
      "metadata": {
        "id": "xNAvEMw8aqbV"
      },
      "id": "xNAvEMw8aqbV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Flower classification training and evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}